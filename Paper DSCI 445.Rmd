---
title: "Paper DSCI 445 Project"
author: "Juliette Dashe, Tanner Halopoff, Munisa Yaqub, Leah Cordova"
date: "`r Sys.Date()`"
output: 
  pdf_document:
    number_sections: true
    toc: true
  html_document:
    theme: journal
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE, fig.align = 'center')
library(ggplot2)

```
RF
So far after doing some base set up and coding, like loading in my packages which were the randomForest and dplyr, we all begin cleaning the data which had some similarities and also caused us to branch off at certain points because certain models required their own niche cleaning and set up. I just made sure that all invalid rows were removed and everything was set up as a factor if applicable.

Then after setting the seed and partitioning the dat it was time to set up the random forest model itself. I did some research and looked at notes before coming to the conclusion that 500 trees would be the best because it is difficult to over fit with this model, yet with runtimes being extensive, 500 is a sweet spot.

5 fold cross validation was taking a lot of time to run and I am still dialing it in along with the relevant plots. 

What I have done with the training and test split is obtain the accuracy of my prediction based on the one split that I did. This resulted in 83.3% success in predicting an individual's factor of depressed or not. 

A large part of random forest is also to see the importance of the variables and we can run just a couple of tweaks to get a mean decrease in prediction accuracy when a variable is removed and therefore we see home large of a contributor to success it was. Next I will do this with 5 fold cross validation to see some more certainty and repetition.


# Motivation

motivation

# Methodology
## KNN

```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("images/KNN_numeric_Categorical.png")
```
```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("images/KNN_numeric.png")
```
We included K-nearest neighbors because it is a simple, non-parametric classification method that does not assume a linear relationship between the predictors and Depression. KNN predicts a student’s depression status by looking at the k most similar students in the training data (based on age, academic pressure, sleep, stress, etc.) and using a majority vote. This “similar students → similar outcome” idea is very natural for our problem and provides a useful baseline to compare with logistic regression, lasso, and random forest.

For K-nearest neighbors we first used only the numeric predictors (Age, Academic Pressure, Work Pressure, CGPA, Study Satisfaction, Job Satisfaction, Work/Study Hours, Financial Stress). Using 5-fold cross-validation, the CV accuracy was about 0.76 for k = 5 and 0.77 for k = 10, so we chose k = 10 as our main KNN model.

Then we repeated KNN after adding categorical predictors (for example gender, sleep duration, dietary habits, suicidal thoughts, family history) and turning them into dummy 0/1 variables. The 5-fold CV accuracies for k = 5 and k = 10 were very similar to the numeric-only model, with only a small change in accuracy. This suggests that adding the categorical variables did not dramatically improve KNN performance for this dataset, and most of the signal is already captured by the numeric predictors.

# Results
# graphs to the moon and back, advised to download any graphs made and 
# link them here instead of doing any coding 

# References
Adil Shamim, Student Depression Dataset, kaggle.com

