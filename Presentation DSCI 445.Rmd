---
title: "Depression Classification Project"
subtitle: "DSCI 445 Final Project"
author: "Juliette Dashe, Tanner Halopoff, Munisa Yaqub, Leah Cordova"
date: "`r Sys.Date()`"
output: 
  ioslides_presentation:
    widescreen: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```


## How Well Can We Classify Student Depression?

* <span style="color: blue"> **Data Set** </span>

Student Depression Dataset

* <span style="color: blue">**Data**</span>

Almost 28,000 student records with various academic, financial, and lifestyle predictors


* <span style="color: blue">**Goal**</span>

Classify student depression risk with high accuracy and interpretability 

* <span style="color: blue">**Methods Used** </span>

Logistic Regression, K Nearest Neighbors, Lasso and Random Forest

## Specificities

**Categories:**

* Gender, Age, City, Academic Pressure, CGPA, Study Satisfaction, Sleep Duration, Dietary Habits, Academic Level, Have You Ever Had Suicidal Thoughts?, Financial Stress, Family History of Mental Illness, and **Depression**

* 13 Categories

* Removed ID, Profession, Work Pressure and Job Satisfaction


## Descriptive Graphs



## Slide 2: The Results - Tanner

Key Takeaways

- **Random Forests can make accurate predictions** for student depression  
- **Trying multiple mtry values** (features per split) improves performance,  
  though it increases computation time  
- **Variable importance is highly intuitive and useful** for interpretation  
    - Suicidal Thoughts  
    - Academic Pressure  
    - Age  
    - Study Hours  
- **Cross-validation accuracy exceeded 83.5%**



## Slide 3: The Results - Munisa


## Juliette Lasso

**Why Lasso:** 

Addressing overfitting and difficult interpretation, essentially logistic regression with an added L1 regularization penalty, due to high dimensionality

##
**The How**

* Transformed all categorical features into numeric dummy variables
* Standardized all numeric predictors so Lasso penalty will treat them equally
* Performed 5-fold CV to tune the penalty parameter, lambda, across 30 levels

```{r, echo=FALSE, out.width="40%", fig.align='center'}
knitr::include_graphics("images/recipe.png")
```

##
**The Result**

* Chose the lambda value ($\lambda$) .00174 based on the **Receiver Operating Characteristic Area Under the Curve (ROC AUC)**, which yielded a peak CV performance of around 92%.

* Model set **28 coefficients to 0** and resulted in a sparser final model with 
**40 relevant features.**

* Some takeaways: For every standard deviation increase in Academic Pressure, the odds of a student experiencing depression increase by over $\mathbf{200\%}$ (a factor of $3.06$).

* For every one standard deviation increase in Study Satisfaction, the odds of experiencing depression decrease by about $27\%$. 


## Nice Graph!

```{r, echo=FALSE, out.width="80%", fig.align='center'}
knitr::include_graphics("images/Lasso Image.png")
```




## Slide 4: The Results - Leah

## Slide 5: Conclusion
