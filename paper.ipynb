{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSCI445 Term Project Paper - Bank Account Fraud Detection\n",
    "### Jakob Wickham, Nick Brady, Noah Sturgeon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import imblearn as skim\n",
    "import xgboost as xgb\n",
    "import kagglehub\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve, classification_report\n",
    "\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from imblearn.combine import SMOTETomek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{Imporanted of fraud dection in finacial service (Provide reference)}\n",
    "-{Challanges associated with fraud dection: Imbalced datsets , cost associated with false postive / false negatives}\n",
    "-{Objective of the proeject}:Our goal in this project is to work with various machine learning methods to get the highest recall (how many fraudulent transactions are successfully identified) possible, while locking our false positive rate (how many valid transactions are misidentified) at 5% or lower. The reason for using recall, as opposed to accuracy, is due to the nature of the dataset. There is far less fraud than legitimate transactions, and so aiming for accuracy will inherently create a model that is useless at detecting fraud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{Description of the dataset}:\n",
    "    - Source of the data set \n",
    "    - {Key Features} \n",
    "    - {Target Variables}\n",
    "\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;The Bank Account Fraud NeurIPS 2022 datasets (called BAF for short) are a suite of synthetic datasets meant to evaluate machine learning methods. \n",
    "BAF was generated with a fraud label, a boolean value called fraud_bool, which is the response on which machine learning methods can be tested on.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodolgy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{handling missing values}\n",
    "-{removing or inputing outliers}\n",
    "-{flagging inputed values}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only run if you want to locally have the dataset on your machine\n",
    "path = kagglehub.dataset_download(\"sgpjesus/bank-account-fraud-dataset-neurips-2022\")\n",
    "\n",
    "data: pd.DataFrame = pd.read_csv(f\"{path}/Base.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{Checking for missing values there is none but based on variable description some missing could be hideen due to -1 } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_data_summary = pd.DataFrame({\n",
    "    'Column': data.columns,\n",
    "    'Missing_Count': data.isnull().sum(),\n",
    "    'Missing_Percentage': data.isnull().mean() * 100\n",
    "}).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{checking for negative values in the columns that state that negative are missing, due to the high percentage of missing we are dropping those speicific columns}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              Missing Data Count  Percentage Missing\n",
      "prev_address_months_count                 712920             71.2920\n",
      "current_address_months_count                4254              0.4254\n",
      "intended_balcon_amount                    742523             74.2523\n",
      "bank_months_count                         253635             25.3635\n",
      "session_length_in_minutes                   2015              0.2015\n",
      "device_distinct_emails_8w                    359              0.0359\n"
     ]
    }
   ],
   "source": [
    "numeric_missing_value_columns = [\n",
    "    'prev_address_months_count',\n",
    "    'current_address_months_count',\n",
    "    'intended_balcon_amount', \n",
    "    'bank_months_count', \n",
    "    'session_length_in_minutes', \n",
    "    'device_distinct_emails_8w'\n",
    "]\n",
    "\n",
    "# Create a summary DataFrame for missing data\n",
    "missing_data_summary = pd.DataFrame({\n",
    "    'Missing Data Count': [(data[col] < 0).sum() for col in numeric_missing_value_columns],\n",
    "    'Percentage Missing': [(data[col] < 0).mean() * 100 for col in numeric_missing_value_columns]\n",
    "}, index=numeric_missing_value_columns)\n",
    "\n",
    "print(missing_data_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-{What is imputation and what we are dong it, why is flagging the inputed data so important}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data = data.copy()\n",
    "\n",
    "# Removing the 'device_fraud_count' column due to it only containing one value\n",
    "cleaned_data = cleaned_data.drop('device_fraud_count', axis=1)\n",
    "\n",
    "# Removing the 'prev_address_months_count' and 'intended_balcon_amount' columns due to high missing data\n",
    "cleaned_data = cleaned_data.drop(['prev_address_months_count', 'intended_balcon_amount'], axis=1)\n",
    "\n",
    "# List of columns to impute with the median\n",
    "columns_to_impute = [\n",
    "    \"current_address_months_count\", \n",
    "    \"bank_months_count\", \n",
    "    \"session_length_in_minutes\", \n",
    "    \"device_distinct_emails_8w\"\n",
    "]\n",
    "\n",
    "# Impute missing values and create an is_imputed flag\n",
    "for col in columns_to_impute:\n",
    "    imputed_flag_col = f\"{col}_is_imputed\"\n",
    "    # Create a flag column to indicate imputed values\n",
    "    cleaned_data[imputed_flag_col] = cleaned_data[col] < 0\n",
    "    # Impute missing values (negative values treated as missing) with the median\n",
    "    median_value = cleaned_data.loc[cleaned_data[col] >= 0, col].median()\n",
    "    cleaned_data[col] = cleaned_data[col].where(cleaned_data[col] >= 0, median_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
