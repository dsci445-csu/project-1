---
title: "Juliette's document"
author: "Juliette Dashe"
date: "2025-11-29"
output: html_document
---
```{r}
#packages
library(tune)
library(readr)
library(dplyr)
library(stringr)
library(tidymodels)
library(rsample)
set.seed(445)
```

```{r}
#data cleaning

student_depression_dataset <- read_csv("student_depression_dataset.csv")

student_depression_dataset$id <- NULL
student_depression_dataset$`Work Pressure` <- NULL
student_depression_dataset$`Job Satisfaction` <- NULL
student_depression_dataset$Profession <- NULL

df_clean <- student_depression_dataset |>
  mutate(
    # Making degrees into 5 categories 
    Academic_Level = case_when(
      # High School
      Degree == "'Class 12'" ~ "High School",
      
      # Bachelor's Degrees (including B.Pharm, B.Tech, MBBS, etc.)
      Degree %in% c("BE", "B.Arch", "BHM", "B.Com", "BSc", "B.Ed", "LLB", 
                    "B.Pharm", "B.Tech", "BA", "BBA", "BCA") ~ "Bachelor's",
      
      # Master's Degrees/Professional Doctorates
      Degree %in% c("MA", "MBA", "MBBS", "MCA", "LLM", "MD", "M.Com", 
                    "ME", "M.Ed", "MHM", "M.Pharm", "MSc", "M.Tech", "MBBS") ~ "Master's",
      
      # PhD
      Degree == "PhD" ~ "PhD",
      
      # Other/Unspecified
      Degree == "Others" ~ "Other",
      
      # Catch-whatever else I might be missing 
      TRUE ~ "Unknown"
    ),
    
    # Convert the new column to an ordered factor for consistent analysis
    Academic_Level = factor(Academic_Level, 
                            levels = c("High School", "Bachelor's", "Master's", "PhD", "Other", "Unknown"))
  ) |>
  select(-Degree)

cat("Frequency of new Academic Levels:\n")
df_clean |>
  count(Academic_Level) |>
  print()


```

```{r}
#cross validation time 
#first data split
df_clean <- df_clean %>%
  mutate(Depression = factor(Depression))
data_split <- initial_split(df_clean, prop = 0.8, strata = Depression)
train_data <- training(data_split)
test_data <- testing(data_split)



#define recipe
lasso_recipe <- recipe(Depression ~ ., Degree, data = train_data) |>
  
  # 1. Impute missing values for all numeric predictors 
  step_impute_median(all_numeric_predictors()) |> 
  
  # 2. Handle 'novel' levels by grouping them 
  step_novel(all_nominal_predictors()) |> 
  
  # 3. Convert factors and characters to binary columns
  step_dummy(all_nominal_predictors()) |>
  
  # 4. Removing zero variance columns
  step_zv(all_predictors()) |>
  
  # 5. Scale predictors
  step_normalize(all_predictors())

#model specification Lasso, mixture = 1
lasso_spec <- logistic_reg(
  penalty = tune(), #placeholder for penalty parameter
  mixture  = 1
) |>
  set_mode("classification") |>
  set_engine("glmnet")


#5 fold CV Resamples
data_folds <- vfold_cv(train_data, v = 5, strata = Depression)

#define tuning grid
lambda_grid <- grid_regular(penalty(), levels = 30)

#workflow
lasso_wf <- workflow() |>
  add_model(lasso_spec) |>
  add_recipe(lasso_recipe)

#results
lasso_tuning_results <- tune_grid(
  lasso_wf,
  resample = data_folds,
  grid = lambda_grid,
  metrics = metric_set(roc_auc, accuracy, sens, spec),
  control = control_grid(save_pred = TRUE)
)
```


```{r}
#results
show_best(lasso_tuning_results, metric = "roc_auc")

#now time to extract the optimal configuration and fit final model
best_lasso_penalty <- select_best(lasso_tuning_results, metric = "roc_auc")

# 2. Finalize the workflow by plugging the best penalty into the model specification
final_lasso_wf <- finalize_workflow(lasso_wf, best_lasso_penalty)

# 3. Fit the final model to the ENTIRE training set
final_fit_results <- fit(final_lasso_wf, data = train_data)

# 4. Display the non-zero coefficients of the final model 
final_coefs <- tidy(final_fit_results) %>%
  filter(term != "(Intercept)", estimate != 0)

print(final_coefs)

##finding out how many predictors there are after dummy encoding
# 1. Prepare the recipe: Calculate all means, medians, and identify all category levels.
final_prepared_recipe <- prep(lasso_recipe, training = train_data)

# 2. Bake the recipe: Apply all the steps (imputation, dummy creation, scaling) 
#    to the training data to create the final design matrix.
final_design_matrix <- bake(final_prepared_recipe, new_data = train_data, 
                            all_predictors())

# 3. Count the columns: The number of columns is the exact number of predictors (P)
#    your Lasso model sees.
total_predictors_verified <- ncol(final_design_matrix)

cat("## Verified Total Predictors (P) in Final Design Matrix:\n")
cat(total_predictors_verified, "predictors")

```

```{r}
#plot

library(ggplot2)
library(tune)
library(dplyr)

# 1. Extract the raw metric data from the tuning results
# This creates a standard data frame (tibble) that is easy to manipulate.
plot_data <- collect_metrics(lasso_tuning_results) %>%
  filter(.metric == "roc_auc") %>%
  # Ensure penalty is logarithmic for plotting scale
  mutate(log_penalty = log(penalty))

# 2. Extract the optimal penalty value for highlighting
best_penalty_log <- log(select_best(lasso_tuning_results, metric = "roc_auc")$penalty)


# 3. Create the plot using ggplot2 (Robust Method)
final_tuning_plot <- ggplot(plot_data, aes(x = log_penalty, y = mean)) +
  geom_line(color = "#1F78B4") +
  geom_point(color = "#1F78B4") +
  
  # Highlight the optimal penalty chosen by CV
  geom_vline(xintercept = best_penalty_log, linetype = "dashed", color = "red", linewidth = 1) +
  
  # Add labels and theme
  labs(
    title = "Lasso Logistic Regression Tuning: ROC AUC vs. Penalty",
    subtitle = paste("Optimal Penalty (lambda):", round(exp(best_penalty_log), 5)),
    x = expression(Log(lambda)), # Uses LaTeX for lambda symbol
    y = "Mean ROC AUC"
  ) +
  theme_minimal(base_size = 14)

# 4. Display the plot
final_tuning_plot
```


```{r}
final_coefs_tidy <- tidy(final_fit_results) %>%
  dplyr::filter(term != "(Intercept)", abs(estimate) > 1e-10)
# 1. Calculate the number of relevant features (non-zero coefficients)
relevant_features <- nrow(final_coefs_tidy)

# 2. Total predictors the model saw (The P after dummy encoding)
# We use the previously calculated estimate of ~72 total features.
total_predictors <- 68

# 3. Calculate the number of features set to zero (Dropped)
dropped_features <- total_predictors - relevant_features

cat("## Lasso Model Sparsity Calculation\n")
cat("-----------------------------------\n")
cat("A. Features Retained (Non-Zero Coefficients): ", relevant_features, "\n")
cat("B. Features Dropped (Set to Zero): ", dropped_features, "\n")
```





